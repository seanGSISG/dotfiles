# CLAUDE.md: NVIDIA DGX Spark Environment

<dependencies>
@MCP.md
@SSH.md
@.aidocs/repos/index.md
</dependencies>

<working_directories>
- When cloning a repo or starting a new project, always clone or create new project folders in `/home/adminuser/dev/github/<project-name>`
<IMPORTANT>
- You MUST keep an index of all projects, repos, etc in `/home/adminuser/dev/claude-home/.aidocs/repos`
- When asked to save a plan file, copy to `/home/adminuser/dev/claude-home/.aidocs/plans/<MM-DD>-<3-to-4-word-description>.md`
- Standard repo/project template
```txt
/home/adminuser/dev/github/<project-name>/
â”œâ”€â”€ .aidocs/  # contains organized subfolders for ai-agent documentation about the codebase/sdk info/research etc
â””â”€â”€ CLAUDE.md # uses Anthropic best practices for CLAUDE.md - a living document that is updated constantly and uses progressive disclosure to point to more detailed docs in `.aidocs/`
```
</IMPORTANT>
</working_directories>

<hardware_context>
<system>NVIDIA DGX Spark - Desktop AI Supercomputer (Blackwell Architecture)</system>

| Component | Spec | Implication |
|-----------|------|-------------|
| SoC | GB10 Grace Blackwell Superchip | Unified CPU+GPU architecture |
| CPU | 20-core Arm (10 Perf / 10 Efficiency) | Hybrid workload optimization |
| GPU | Blackwell SM120 | Native NVFP4/MXFP4 support |
| Memory | 128 GB Unified LPDDR5x | Coherent pool - hard limit for models+KV cache |
| Bandwidth | 273 GB/s | High-throughput inference capable |
| Interconnect | 200 Gbps ConnectX-7 (RoCE/InfiniBand) | Cluster-ready via TP |
</hardware_context>

<inference_engine_selection>
<decision_matrix>
| Use Case | Engine | Rationale |
|----------|--------|-----------|
| Single-user, low-latency | llama.cpp | ~60 t/s on 120B models (small context) |
| Multi-agent / parallel | vLLM | Optimized concurrent throughput |
| Cluster deployment | vLLM | Tensor Parallel = ~2x scaling on dense models |
| New model architectures | vLLM | Day-1 support vs months for llama.cpp |
| MXFP4 speed priority | SGLang | Triton MXFP4 kernels = 53 t/s |
| Large context processing | vLLM | Faster prompt processing + Responses endpoint |
</decision_matrix>

<engine_limitations>
- llama.cpp: MXFP4 Blackwell kernels NOT ready in main branch; Layer Split clustering causes performance LOSS (no RoCE/IB support)
- vLLM: Higher baseline latency than llama.cpp for single-user
- SGLang: Requires specialized Spark builds for Triton MXFP4
</engine_limitations>
</inference_engine_selection>

<quantization_guidance>
<current_state>Blackwell NVFP4/MXFP4 software implementation in flux</current_state>

| Quant Type | Throughput | ITL | When to Use |
|------------|------------|-----|-------------|
| AWQ 4-bit | Baseline | ~39ms | Production default in vLLM |
| NVFP4 | -18-32% vs AWQ | ~51ms | Only when model fit > speed |

<rule>Default to AWQ unless memory-constrained beyond 128GB coherent pool.</rule>
</quantization_guidance>

<operational_rules priority="enforce">
1. **Memory**: Never exceed 128GB physical limit for model weights + KV cache. Avoid swap.
2. **Benchmarking**: Disable prompt caching when using `vllm bench serve --sweep` or results invalid.
3. **Concurrency**: Route parallel/agentic workloads to vLLM or SGLang, not llama.cpp.
4. **Clustering**: Use vLLM with Tensor Parallel only. llama.cpp clustering degrades performance.
</operational_rules>

<progressive_disclosure>
<behavior>Read referenced files only when task requires that domain knowledge.</behavior>
- PERFORMANCE.md â†’ Engine benchmarks, tuning parameters, comparative analysis
</progressive_disclosure>



<claude-mem-context>
# Recent Activity

<!-- This section is auto-generated by claude-mem. Edit content outside the tags. -->

### Jan 6, 2026

| ID | Time | T | Title | Read |
|----|------|---|-------|------|
| #65 | 10:59 PM | ðŸ”µ | Identify encrypted SSH private key file | ~177 |
</claude-mem-context>